'''
name: run_xgboost_gap_2025.py
usage: Train XGBoost on 2010-2014, Predict 2025 
       ä½¿ç”¨ä¸ƒä¸ªå˜é‡è®­ç»ƒxgboost
input: /root/autodl-tmp/project/mlpj/data/processed/train_2010_2014_rich.npy
       /root/autodl-tmp/project/mlpj/data/processed/test_2025_rich.npy
output: /root/autodl-tmp/project/mlpj/results/logs/xgboost_gap_2025_results.npz
        /root/autodl-tmp/project/mlpj/results/figures/xgboost_gap_2025.png
'''

import os
import random
import numpy as np
import pandas as pd
import xgboost as xgb
import joblib
from sklearn.preprocessing import MinMaxScaler
from sklearn.multioutput import MultiOutputRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from tqdm import tqdm

# ================= 0. æ ‡å‡†åŒ–åè®®ï¼šéšæœºç§å­ =================
def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    print(f"ğŸ”’ [Protocol] Random Seed set to {seed}")

set_seed(42)

# ================= 1. é…ç½®åŒºåŸŸ =================
# è·¯å¾„ 
train_npy = "/root/autodl-tmp/project/mlpj/data/processed/train_2010_2014_rich.npy"
test_npy  = "/root/autodl-tmp/project/mlpj/data/processed/test_2025_rich.npy"
test_csv  = "/root/autodl-tmp/project/mlpj/data/processed/test_2025_rich.csv"

# ä¿å­˜è·¯å¾„
save_dir = "/root/autodl-tmp/project/mlpj/results/logs"
os.makedirs(save_dir, exist_ok=True)
output_npz = os.path.join(save_dir, "xgboost_gap_2025_results.npz")
output_img = "/root/autodl-tmp/project/mlpj/results/figures/xgboost_gap_2025.png"

# å‚æ•°
context_len = 512
pred_len = 24
input_dims = 7 # 7ä¸ªæ°”è±¡ç‰¹å¾

# ================= 2. æ•°æ®åŠ è½½ä¸é¢„å¤„ç† =================
print("1. Loading Data...")
if not os.path.exists(train_npy) or not os.path.exists(test_npy):
    print("âŒ æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥ processed æ–‡ä»¶å¤¹")
    exit()

train_data = np.load(train_npy) # (N_train, 7)
test_data  = np.load(test_npy)  # (N_test, 7)

print(f"   Train Shape: {train_data.shape} (2010-2014)")
print(f"   Test Shape:  {test_data.shape} (2025)")

# å½’ä¸€åŒ– (å…³é”®: åªåœ¨è®­ç»ƒé›†ä¸Š Fit)
scaler = MinMaxScaler((0, 1))
scaler.fit(train_data)

train_scaled = scaler.transform(train_data)
test_scaled  = scaler.transform(test_data)

# ================= 3. æ„é€  Flatten æ•°æ®é›† =================
# XGBoost æ— æ³•ç›´æ¥åƒ (512, 7) çš„ 3D æ•°æ®ï¼Œéœ€è¦å±•å¹³ä¸º (512*7) çš„ 1D å‘é‡
def create_xgb_dataset(data, context_len, pred_len, stride=1):
    X, y = [], []
    for i in range(0, len(data) - context_len - pred_len + 1, stride):
        # Input: è¿‡å» 512 å°æ—¶çš„ 7 ä¸ªç‰¹å¾ -> Flatten
        # Shape: (512 * 7,) = (3584,)
        feature_vec = data[i : i+context_len].flatten()
        
        # Output: æœªæ¥ 24 å°æ—¶çš„æ°”æ¸© (ç¬¬0åˆ—)
        target = data[i+context_len : i+context_len+pred_len, 0]
        
        X.append(feature_vec)
        y.append(target)
    return np.array(X), np.array(y)

print("2. Constructing Training Samples...")
# è®­ç»ƒæ—¶ stride=12 (é™é‡‡æ ·ä»¥èŠ‚çœå†…å­˜å’Œæ—¶é—´ï¼Œä½†ä¿è¯æ ·æœ¬é‡è¶³å¤Ÿ)
# å¦‚æœå†…å­˜åªæœ‰ 35GBï¼Œ3584ç»´ç‰¹å¾ x æ•°ä¸‡æ ·æœ¬æ˜¯å¯ä»¥åƒä¸‹çš„
X_train, y_train = create_xgb_dataset(train_scaled, context_len, pred_len, stride=12)

print(f"   -> Feature Dims: {X_train.shape[1]} (512 * 7)")
print(f"   -> Samples: {X_train.shape[0]}")

# ================= 4. è®­ç»ƒ XGBoost =================
print("3. Training XGBoost (MultiOutput)...")

# é…ç½® GPU
xgb_params = {
    'n_estimators': 800,
    'learning_rate': 0.05,
    'max_depth': 8,             # æ ‘æ·±ä¸€ç‚¹ï¼Œå› ä¸ºç‰¹å¾ç»´åº¦å¾ˆé«˜
    'objective': 'reg:squarederror',
    'n_jobs': -1,
    'tree_method': 'hist',      # å¿…é¡»ç”¨ hist æ¨¡å¼åŠ é€Ÿ
    'device': 'cuda',           # ä½¿ç”¨ GPU
    'colsample_bytree': 0.6     # æ¯æ¬¡åªç”¨ 60% çš„ç‰¹å¾ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
}

model = MultiOutputRegressor(xgb.XGBRegressor(**xgb_params))
model.fit(X_train, y_train)
print("   âœ… Training Complete.")

# ================= 5. é¢„æµ‹ 2025 (Gap Test) =================
print("4. Predicting 2025 (Gap Test)...")

all_preds = []
all_trues = []
timestamps = []

# ç”Ÿæˆ 2025 æ—¶é—´è½´
if os.path.exists(test_csv):
    df_test = pd.read_csv(test_csv)
    time_index = pd.to_datetime(df_test['time'])
else:
    time_index = pd.date_range(start="2025-01-01", periods=len(test_data), freq='H')

# æ»šåŠ¨é¢„æµ‹
# æ­¥é•¿ 24
idx_iter = range(0, len(test_scaled) - context_len - pred_len, 24)

# å‡†å¤‡æµ‹è¯•æ•°æ® (é¢„å…ˆæ„å»ºå¥½å¯ä»¥åŠ é€Ÿï¼Œä½†ä¸ºäº†çœå†…å­˜è¿˜æ˜¯å¾ªç¯æ„å»º)
for i in tqdm(idx_iter, desc="Inferencing"):
    # A. å‡†å¤‡è¾“å…¥
    # å– [t-512 : t] å¹¶å±•å¹³
    ctx = test_scaled[i : i+context_len].flatten().reshape(1, -1)
    
    # B. æ¨ç†
    pred_norm = model.predict(ctx).flatten()
    
    # C. çœŸå®å€¼
    true_norm = test_scaled[i+context_len : i+context_len+pred_len, 0]
    
    # D. åå½’ä¸€åŒ–
    # æ„é€  Dummy çŸ©é˜µ
    dummy_pred = np.zeros((pred_len, input_dims))
    dummy_pred[:, 0] = pred_norm
    pred_real = scaler.inverse_transform(dummy_pred)[:, 0]
    
    dummy_true = np.zeros((pred_len, input_dims))
    dummy_true[:, 0] = true_norm
    true_real = scaler.inverse_transform(dummy_true)[:, 0]
    
    all_preds.extend(pred_real)
    all_trues.extend(true_real)
    timestamps.extend(time_index[i+context_len : i+context_len+pred_len])

all_preds = np.array(all_preds)
all_trues = np.array(all_trues)

# ================= 6. è¯„ä¼°ä¸ä¿å­˜ =================
mse = mean_squared_error(all_trues, all_preds)
mae = mean_absolute_error(all_trues, all_preds)
rmse = np.sqrt(mse)
r2 = r2_score(all_trues, all_preds)

print("\n" + "="*40)
print(f"ğŸ“Š XGBoost 2025 Evaluation (Gap Strategy):")
print(f"   MAE  : {mae:.4f} Â°C")
print(f"   RMSE : {rmse:.4f} Â°C")
print(f"   RÂ²   : {r2:.4f}")
print("="*40)

# ä¿å­˜ NPZ
np.savez(output_npz, preds=all_preds, trues=all_trues, mse=mse, mae=mae, rmse=rmse, r2=r2)
print(f"ğŸ’¾ Results saved to {output_npz}")

