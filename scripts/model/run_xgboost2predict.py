'''
name: run_xgboost2predict.py
usage: use XGBoost to train and predict
input: /root/autodl-tmp/project/mlpj/data/processed/shanghai_2010_2025_full.npy
       /root/autodl-tmp/project/mlpj/data/processed/shanghai_2010_2025.csv
output: /root/autodl-tmp/project/mlpj/results/logs/xgboost_2025_standard_results.npz
authors/date: Q/2025.12.16
'''

import os
import random
import numpy as np
import pandas as pd
import xgboost as xgb
from sklearn.preprocessing import MinMaxScaler
from sklearn.multioutput import MultiOutputRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from tqdm import tqdm

# ================= 0. æ ‡å‡†åŒ–åè®®ï¼šéšæœºç§å­ =================
def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    print(f"[Protocol] Random Seed set to {seed}")

set_seed(42)

# ================= 1. é…ç½®åŒºåŸŸ =================
# è¾“å…¥è·¯å¾„
data_npy = "/root/autodl-tmp/project/mlpj/data/processed/shanghai_2010_2025_full.npy"
data_csv = "/root/autodl-tmp/project/mlpj/data/processed/shanghai_2010_2025.csv"

# è¾“å‡ºè·¯å¾„
save_dir = "/root/autodl-tmp/project/mlpj/results/logs"
os.makedirs(save_dir, exist_ok=True)
output_npz = os.path.join(save_dir, "xgboost_2025_standard_results.npz")
output_img = "xgboost_2025_standard.png"

# æ ‡å‡†åŒ–å‚æ•°
context_len = 512  # è¾“å…¥è¿‡å» 512 å°æ—¶
pred_len = 24      # é¢„æµ‹æœªæ¥ 24 å°æ—¶
stride = 24        # æ»šåŠ¨æ­¥é•¿ (æµ‹è¯•æ—¶)

# ================= 2. æ•°æ®åŠ è½½ä¸æ ‡å‡†åŒ–é¢„å¤„ç† =================
print("1. [Protocol] Loading & Preprocessing Data...")
if not os.path.exists(data_npy) or not os.path.exists(data_csv):
    print("âŒ Data not found.")
    exit()

# åŠ è½½åŸå§‹æ•°æ®
data_values = np.load(data_npy) # (N,)
if data_values.ndim == 1:
    data_values = data_values.reshape(-1, 1)

df = pd.read_csv(data_csv)
df['time'] = pd.to_datetime(df['time'])

# --- è¾…åŠ©ç‰¹å¾ï¼šæ—¶é—´ Embedding (XGBoost éœ€è¦è¿™ä¸ªæ¥æ„ŸçŸ¥å­£èŠ‚) ---
# è¿™ä¸è¿å"åŒä¸€å¥—æ•°æ®é›†"åŸåˆ™ï¼Œå› ä¸ºè¿™æ˜¯ä»æ—¶é—´æˆ³ä¸­æå–çš„å›ºæœ‰ä¿¡æ¯
df['hour'] = df['time'].dt.hour
df['month'] = df['time'].dt.month
df['dayofyear'] = df['time'].dt.dayofyear
time_feats = df[['hour', 'month', 'dayofyear']].values

# --- ä¸¥æ ¼åˆ’åˆ†è®­ç»ƒé›†ä¸æµ‹è¯•é›† ---
split_date = pd.Timestamp("2025-01-01 00:00:00")
train_mask = df['time'] < split_date

# è®­ç»ƒé›†: 2010-2024
train_data_raw = data_values[train_mask]
train_time_raw = time_feats[train_mask]

# æµ‹è¯•èµ·ç‚¹ç´¢å¼•
test_start_idx = np.sum(train_mask)
print(f"   -> Train Data End: {df['time'].iloc[test_start_idx-1]}")
print(f"   -> Test Start:     {df['time'].iloc[test_start_idx]}")

# --- ç»Ÿä¸€é¢„å¤„ç†: MinMaxScaler (0~1) ---
# è§„åˆ™: åªèƒ½ç”¨è®­ç»ƒé›†(2010-2024) fit
scaler = MinMaxScaler(feature_range=(0, 1))
scaler.fit(train_data_raw)

# è½¬æ¢å…¨é‡æ•°æ®
data_scaled = scaler.transform(data_values).flatten()
print(f"   -> [Protocol] Scaler fitted on Train set (2010-2024).")

# ================= 3. æ„é€ è®­ç»ƒæ ·æœ¬ =================
# æˆ‘ä»¬éœ€è¦æ„é€  (X, y) å¯¹æ¥è®­ç»ƒ XGBoost
# X: [Lag_1 ... Lag_512, Hour, Month, Day]
# y: [Future_1 ... Future_24]

def create_dataset(data_seq, time_seq, context_len, pred_len, stride=1):
    X, y = [], []
    # è®­ç»ƒæ—¶ stride å¯ä»¥å°ä¸€ç‚¹(å¦‚1æˆ–12)ä»¥å¢åŠ æ ·æœ¬é‡
    # è¿™é‡Œè®¾ä¸º 12 ä»¥å…¼é¡¾é€Ÿåº¦å’Œç²¾åº¦
    for i in range(0, len(data_seq) - context_len - pred_len + 1, stride):
        # 1. å†å²æ°”æ¸©ç‰¹å¾ (512ç»´)
        lags = data_seq[i : i + context_len]
        
        # 2. é¢„æµ‹æ—¶åˆ»çš„æ—¶é—´ç‰¹å¾ (3ç»´) - å–é¢„æµ‹çª—å£çš„ç¬¬ä¸€ä¸ªæ—¶é—´ç‚¹
        # å‘Šè¯‰æ¨¡å‹"æˆ‘ä»¬è¦é¢„æµ‹ä»€ä¹ˆæ—¶å€™çš„æ°”æ¸©"
        curr_time = time_seq[i + context_len] 
        
        # åˆå¹¶
        feature_vector = np.concatenate([lags, curr_time])
        
        # 3. ç›®æ ‡ (24ç»´)
        target = data_seq[i + context_len : i + context_len + pred_len]
        
        X.append(feature_vector)
        y.append(target)
    return np.array(X), np.array(y)

print("2. Constructing Training Samples...")
X_train, y_train = create_dataset(
    data_scaled[train_mask], 
    train_time_raw, 
    context_len, 
    pred_len, 
    stride=12 # è®­ç»ƒé‡‡æ ·æ­¥é•¿
)
print(f"   -> Training Samples: {X_train.shape}")

# ================= 4. è®­ç»ƒæ¨¡å‹ =================
print("3. Training XGBoost (MultiOutput)...")
# ä½¿ç”¨ GPU åŠ é€Ÿ
xgb_params = {
    'n_estimators': 800,
    'learning_rate': 0.05,
    'max_depth': 8,
    'objective': 'reg:squarederror',
    'n_jobs': -1,
    'tree_method': 'hist',
    'device': 'cuda'  
}

# MultiOutputRegressor: ä¸€æ¬¡æ€§é¢„æµ‹24ä¸ªç‚¹ï¼Œå¯¹åº” Sundial çš„ generate(24)
model = MultiOutputRegressor(xgb.XGBRegressor(**xgb_params))
model.fit(X_train, y_train)
print("   âœ… Training Complete.")

# ================= 5. æ»šåŠ¨é¢„æµ‹ (Standard Rolling) =================
print(f"4. [Protocol] Rolling Forecast (Stride={stride})...")

all_preds = []
all_trues = []
timestamps = []

# è¿­ä»£å™¨: ä» 2025-01-01 å¼€å§‹
idx_iter = range(test_start_idx, len(data_values) - pred_len, stride)

# å‡†å¤‡æµ‹è¯•æ•°æ® (å…¨é‡å½’ä¸€åŒ–åçš„)
test_data_scaled = data_scaled
test_time_feats = time_feats

for current_idx in tqdm(idx_iter, desc="Inferencing"):
    # A. å‡†å¤‡è¾“å…¥
    # å†å²å½’ä¸€åŒ–æ•°æ®
    history_norm = test_data_scaled[current_idx - context_len : current_idx]
    # å½“å‰é¢„æµ‹ç‚¹çš„æ—¶é—´ç‰¹å¾
    curr_time_feat = test_time_feats[current_idx]
    
    # æ‹¼æ¥è¾“å…¥å‘é‡
    input_vec = np.concatenate([history_norm, curr_time_feat]).reshape(1, -1)
    
    # B. æ¨ç† (å¾—åˆ°å½’ä¸€åŒ–åçš„ 24 å°æ—¶é¢„æµ‹)
    pred_norm = model.predict(input_vec).flatten()
    
    # C. è·å–çœŸå®å€¼
    true_norm = test_data_scaled[current_idx : current_idx + pred_len]
    
    # D. åå½’ä¸€åŒ–
    pred_real = scaler.inverse_transform(pred_norm.reshape(-1, 1)).flatten()
    true_real = scaler.inverse_transform(true_norm.reshape(-1, 1)).flatten()
    
    all_preds.extend(pred_real)
    all_trues.extend(true_real)
    timestamps.extend(df['time'].iloc[current_idx : current_idx + pred_len])

all_preds = np.array(all_preds)
all_trues = np.array(all_trues)

# ================= 6. è®¡ç®—æŒ‡æ ‡ (Standard Metrics) =================
mse = mean_squared_error(all_trues, all_preds)
mae = mean_absolute_error(all_trues, all_preds)
rmse = np.sqrt(mse)
r2 = r2_score(all_trues, all_preds)

print("\n" + "="*40)
print(f"ğŸ“Š XGBoost 2025 Evaluation Report:")
print(f"   MAE  : {mae:.4f} Â°C")
print(f"   RMSE : {rmse:.4f} Â°C")
print(f"   MSE  : {mse:.4f}")
print(f"   RÂ²   : {r2:.4f}")
print("="*40)

# ================= 7. ä¿å­˜ç»“æœ =================
np.savez(output_npz, preds=all_preds, trues=all_trues, mse=mse, mae=mae, rmse=rmse, r2=r2)
print(f"ğŸ’¾ Results saved to {output_npz}")
